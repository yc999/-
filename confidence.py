# -*- coding: utf-8 -*-
# 根据分类结果计算置信度
#   1 先将预测数据排序
#   2 排序完计算置信度
#   3 将最低的置信度对应的预测值对应
#   4 置信度组合 加入集成学习
#

#   1 先将预测数据排序， 实际类型和预测类型需要随着分数一起排序
# score_list = [1,2,3,7,5]  # 分类器得到的分数

# predict_list = [2,3,3,3,3] # 预测的类型

# class_list = [2,3,3,3,3] # 实际的类型


import numpy as np
from sklearn import svm
# 单特征双向LSTM 16 不去重
mybilstmmodel_16_pred_savepath = "/public/ycdswork/lstm_pred/mybilstmmodel_16_pred.npy"
mybilstmmodel_16_pred = np.load(mybilstmmodel_16_pred_savepath)

# 双特征双向LSTM 16 不去重
pred_savepath = "/public/ycdswork/lstm_pred/bilstmmodel_16_pred.npy"
bilstmmodel_16_pred = np.load(mybilstmmodel_16_pred_savepath)

#单特征单LSTM 去重16
# get_my_lstm_model_16_nofull_pred
# lstm_model_1_nofull_16_pred
# lstm++++++++++++++++=====================================
lstm_score_list = []
lstm_predict_list = []
# all_class_list = []
# svm_y_test


for i in range(len(pred_y)):
    indextmp = pred_y[i]
    lstm_score_list.append(pred_y_proba[i][indextmp])
    lstm_predict_list.append(indextmp)


sort_list = []
for i in range(len(lstm_score_list)):
    tmplist = []
    tmplist.append(lstm_score_list[i])    # 预测得分
    tmplist.append(lstm_predict_list[i]) # 预测类别
    tmplist.append(svm_y_test[i])        #实际类别
    sort_list.append(tmplist)


# 按预测得分大小排序
def takeFirst(elem):
    return elem[0]

sort_list.sort(key=takeFirst,reverse = True)
print("sort_list")
# print(sort_list)



#   2 排序完计算置信度

# score_list = [1,2,3,7,5]  # 分类器得到的分数
# predict_list = [2,3,3,3,3] # 预测的类型
# class_list = [2,3,3,3,4] # 实际的类型

lstm_sort_score_list = []
for tmp_list in sort_list:
    lstm_sort_score_list.append(tmp_list[0])

lstm_sort_predict_list = []
for tmp_list in sort_list:
    lstm_sort_predict_list.append(tmp_list[1])

lstm_sort_class_list = []
for tmp_list in sort_list:
    lstm_sort_class_list.append(tmp_list[2]) 


lstm_count_true_list = []    # 按顺序统计正确预测的数量
count_number = 0
for i in range(len(lstm_sort_score_list)):
    if lstm_sort_predict_list[i] == lstm_sort_class_list[i]:
        count_number = count_number + 1
    lstm_count_true_list.append(count_number)


print("lstm_count_true_list")
print(lstm_count_true_list)  #   排序完的数组


confidence_n = 100  #


svm_confidence_list = []
# 获得置信度
for i in range(len(lstm_sort_score_list)):
    tmpcount = 0
    firstpart = 0
    lastpart = 0
    if i<confidence_n + 1:
        firstpart =  confidence_n - i + lstm_count_true_list[i]
    else:
        firstpart = lstm_count_true_list[i] - lstm_count_true_list[i - confidence_n -1]
    if i + confidence_n >= len(lstm_sort_score_list):
        lastpart = lstm_count_true_list[len(lstm_sort_score_list)-1] - lstm_count_true_list[i]
    else:
        lastpart = lstm_count_true_list[i + confidence_n] - lstm_count_true_list[i]
    tmpconfidence = (firstpart + lastpart)/(2*confidence_n + 1)
    svm_confidence_list.append(tmpconfidence)

print("svm_confidence_list")
print(svm_confidence_list)


















# svm 
svm_score_list = []
svm_predict_list = []
# all_class_list = []
# svm_y_test


for i in range(len(pred_y)):
    indextmp = pred_y[i]
    svm_score_list.append(pred_y_proba[i][indextmp])
    svm_predict_list.append(indextmp)


sort_list = []
for i in range(len(svm_score_list)):
    tmplist = []
    tmplist.append(svm_score_list[i])    # 预测得分
    tmplist.append(svm_predict_list[i]) # 预测类别
    tmplist.append(svm_y_test[i])        #实际类别
    sort_list.append(tmplist)


# 按预测得分大小排序
def takeFirst(elem):
    return elem[0]

sort_list.sort(key=takeFirst,reverse = True)
print("sort_list")
# print(sort_list)



#   2 排序完计算置信度

# score_list = [1,2,3,7,5]  # 分类器得到的分数
# predict_list = [2,3,3,3,3] # 预测的类型
# class_list = [2,3,3,3,4] # 实际的类型

svm_sort_score_list = []
for tmp_list in sort_list:
    svm_sort_score_list.append(tmp_list[0])

svm_sort_predict_list = []
for tmp_list in sort_list:
    svm_sort_predict_list.append(tmp_list[1])

svm_sort_class_list = []
for tmp_list in sort_list:
    svm_sort_class_list.append(tmp_list[2]) 


svm_count_true_list = []    # 按顺序统计正确预测的数量
count_number = 0
for i in range(len(svm_sort_score_list)):
    if svm_sort_predict_list[i] == svm_sort_class_list[i]:
        count_number = count_number + 1
    svm_count_true_list.append(count_number)


print("svm_count_true_list")
print(svm_count_true_list)  #   排序完的数组


confidence_n = 100  #


svm_confidence_list = []
# 获得置信度
for i in range(len(svm_sort_score_list)):
    tmpcount = 0
    firstpart = 0
    lastpart = 0
    if i<confidence_n + 1:
        firstpart =  confidence_n - i + svm_count_true_list[i]
    else:
        firstpart = svm_count_true_list[i] - svm_count_true_list[i - confidence_n -1]
    if i + confidence_n >= len(svm_sort_score_list):
        lastpart = svm_count_true_list[len(svm_sort_score_list)-1] - svm_count_true_list[i]
    else:
        lastpart = svm_count_true_list[i + confidence_n] - svm_count_true_list[i]
    tmpconfidence = (firstpart + lastpart)/(2*confidence_n + 1)
    svm_confidence_list.append(tmpconfidence)

print("svm_confidence_list")
print(svm_confidence_list)









# 处理置信度  找到置信度最低值的分数
#   3 将最低的置信度对应的预测值对应
# score_list = [1,2,3,7,5]  # 分类器得到的分数

import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
import numpy as np
import os

# 定义需要拟合的函数
def func(x, a, b, c):
    return a * np.exp(-b * x) + c

# Define the data to be fit with some noise:
# 用numpy的random库生成干扰
xdata = np.linspace(0, 4, 50)
# print(type(xdata))
# y = func(xdata, 2.5, 1.3, 0.5)
# np.random.seed(1719)
# y_noise = 0.2 * np.random.normal(size=xdata.size)
# ydata = y + y_noise

# xdatapath = "C:/Users/shinelon/Desktop/论文实验图/双向LSTM预测得分.txt"

# ydatapath = "C:/Users/shinelon/Desktop/论文实验图/双特征双向LSTM置信度.txt"

# file_handler =open(xdatapath, mode='r')  
# #2.读取文件内容
# contents = file_handler.readlines()


# yfile_handler =open(ydatapath, mode='r')  
# ycontents = yfile_handler.readlines()
# yy = ycontents[0].strip('\n')

xdata = np.array(svm_sort_score_list)
# print("xdata")
# print(xdata)

ydata = svm_confidence_list
print("ydata")

# print(ydata)
plt.plot(xdata, ydata, 'b-', label='data')
# Fit for the parameters a, b, c of the function func:


popt, pcov = curve_fit(func, xdata, ydata)
print("popt")
print(popt)

plt.plot(xdata, func(xdata, *popt), 'r-',
         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))
# Constrain the optimization to the region of 0 <= a <= 3, 0 <= b <= 1 and 0 <= c <= 0.5:
# 限定范围进行拟合
# popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))
# print("popt")
# print(popt)

# plt.plot(xdata, func(xdata, *popt), 'g--',
#          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
#结果
#[2.55423706 1.35190947 0.47450618]
#[2.43708905 1.         0.35015434]


# 拟合情况
# #双特征双向
poly_bilstm_struct = [-6.07477365e+06,  4.74061625e+07, -1.67696546e+08,  3.55822651e+08,
       -5.05042054e+08,  5.06358642e+08, -3.69196420e+08,  1.98595018e+08,
       -7.91442029e+07,  2.32716974e+07, -4.98745079e+06,  7.62102600e+05,
       -8.00377562e+04,  5.41738498e+03, -2.07696643e+02,  3.47163360e+00]
# 单特征双向
poly_bilstm = [-7.93121578e+05 , 6.56098835e+06, -2.45394828e+07,  5.49341900e+07,
 -8.21391343e+07,  8.67041440e+07, -6.65922899e+07,  3.77898251e+07,
 -1.59150147e+07,  4.94515599e+06, -1.11406830e+06 , 1.76239489e+05,
 -1.85988392e+04,  1.20992744e+03 ,-4.22032210e+01,  7.78057032e-01]
#单特征单向
poly_lstm = [-8.68881807e+06,  7.05801423e+07, -2.60242939e+08,  5.76332912e+08,
 -8.54876330e+08,  8.96749075e+08, -6.84735605e+08,  3.85978993e+08,
 -1.61215639e+08,  4.96547960e+07, -1.11302488e+07,  1.77447368e+06,
 -1.93936968e+05,  1.36698511e+04, -5.53632103e+02,  9.82589429e+00]
#双特征单向
poly_lstm_struct = [ 1.47573111e+07, -1.24168700e+08,  4.76193560e+08, -1.10213898e+09,
  1.71817907e+09, -1.90689393e+09,  1.55274765e+09, -9.42214384e+08,
  4.28400984e+08, -1.45532335e+08,  3.65280114e+07, -6.63222999e+06,
  8.40639254e+05, -7.00120601e+04,  3.41642736e+03, -7.33203976e+01]
# svm
poly_svm =[ 5.54785065e+05, -5.23793140e+06,  2.16707018e+07, -5.23211021e+07,
  8.24724830e+07, -8.97060316e+07 , 6.92482958e+07, -3.83692447e+07,
  1.52370790e+07, -4.27820625e+06,  8.26024110e+05, -1.04518489e+05,
  7.97593730e+03, -3.15727151e+02,  6.65986798e+00, -3.23336414e-02]

# svm_struct
poly_svm_struct =[-6.83272574e+04,  6.75185611e+04,  1.27815975e+06, -5.50535618e+06,
  1.09145717e+07, -1.28767585e+07,  9.69415689e+06, -4.65556505e+06,
  1.30835412e+06, -1.29872196e+05, -4.18165302e+04,  1.74837752e+04,
 -2.75422345e+03,  2.11447884e+02, -5.29402352e+00,  7.20447959e-02]





# 获得得分
bilstm_struct_score_list = []
bilstm_score_list = []

lstm_score_list = []
lstm_struct_score_list = []

svm_score_list = []
svm_struct_score_list = []

for i in range(len(bilstmmodel_16_pred_class)):
    indextmp = bilstmmodel_16_pred_class[i]
    bilstm_struct_score_list.append(bilstmmodel_16_pred[i][indextmp])
    indextmp = mybilstmmodel_16_pred_class[i]
    bilstm_score_list.append(mybilstmmodel_16_pred[i][indextmp])
    indextmp = get_my_lstm_model_16_nofull_pred_class[i]
    lstm_score_list.append(get_my_lstm_model_16_nofull_pred[i][indextmp])
    indextmp = lstm_model_1_nofull_16_pred_class[i]
    lstm_struct_score_list.append(lstm_model_1_nofull_16_pred[i][indextmp])
    indextmp = pred_y[i]
    svm_score_list.append(pred_y_proba[i][indextmp])
    indextmp = pred_y_struct[i] 
    svm_struct_score_list.append(pred_y_proba_struct[i][indextmp])



# 初始化 多项式公式

p1_poly_bilstm_struct = np.poly1d(poly_bilstm_struct)
p1_poly_bilstm = np.poly1d(poly_bilstm)

p1_poly_lstm = np.poly1d(poly_lstm)
p1_poly_lstm_struct = np.poly1d(poly_lstm_struct)

p1_poly_svm = np.poly1d(poly_svm)
p1_poly_svm_struct = np.poly1d(poly_svm_struct)



p1_poly_bilstm_struct_confidence = p1_poly_bilstm_struct(bilstm_struct_score_list) # 也可以使用yvals=np.polyval(z1,x)
p1_poly_bilstm_confidence = p1_poly_bilstm(bilstm_score_list)

p1_poly_lstm_confidence = p1_poly_lstm(lstm_score_list) # 也可以使用yvals=np.polyval(z1,x)
p1_poly_lstm_struct_confidence = p1_poly_lstm_struct(lstm_struct_score_list) # 也可以使用yvals=np.polyval(z1,x)

p1_poly_svm_confidence = p1_poly_svm(svm_score_list) # 也可以使用yvals=np.polyval(z1,x)
p1_poly_svm_struct_confidence = p1_poly_svm_struct(svm_struct_score_list) # 也可以使用yvals=np.polyval(z1,x)



#   4 置信度组合 加入集成学习

#双特征双向lstm 阈值   bilstmmodel_16_pred_class    bilstmmodel_16_pred
bilstm_struct_score_limit_value = [0.98, 0.92, 0.66, 0.469]

#单特征双向lstm 阈值   mybilstmmodel_16_pred_class   mybilstmmodel_16_pred

bilstm_score_limit_value = [0.974,0.876, 0.607, 0.399]

#单特征lstm 阈值    get_my_lstm_model_16_nofull_pred_class   get_my_lstm_model_16_nofull_pred

lstm_score_limit_value = [0.987, 0.867,0.70,0.417]

#双特征单向lstm 阈值   lstm_model_1_nofull_16_pred_class  lstm_model_1_nofull_16_pred
lstm_struct_score_limit_value = [0.983, 0.817, 0.631, 0.445]

#svm 阈值        pred_y  pred_y_proba
svm_score_limit_value = [0.961, 0.605, 0.401, 0.208]

#双特征svm 阈值   pred_y_struct  pred_y_proba_struct
svm_struct_score_limit_value = [0.944, 0.572, 0.368, 0.182]


C_list = [0.95, 0.80, 0.60, 0.35, -1]  # 置信度阈值

confidence_list =   [0.7,   0.7,   0.7,   0.7,   0.7,   0.6, 0.5, 0.5,  0.4 ]      # 每个分类器预测的置信度
confidence_class_list =   [1,     2,     3,     3,     4,     5,   5,   6,    7]    # 每个分类器预测的类别

def return_ensemble_result(Clist, bilstm_struct_score, bilstm_score, svm_score, svm_struct_score, class_list):
    #根据得分选择
    result_list = []
    for i in range(len(C_list)):
        if bilstm_struct_score > bilstm_struct_score_limit_value[i]:
            result_list.append()

#   4 置信度组合 加入集成学习
def result_vote(C_list, confidence_list, confidence_class_list):
    decision_list = []
    decision_dic = {}   #  字典形式  { 类型 ：[计数, 该置信度相加的和] }
    for c in C_list:
        for i in range(len(confidence_list)):
            if confidence_list[i] >= c:
                decision_list.append(i)      #   将置信度加入
        if len(decision_list)>2:
            for class_index in decision_list:
                if confidence_class_list[class_index] in decision_dic:
                    decision_dic[confidence_class_list[class_index]][0] = decision_dic[confidence_class_list[class_index]][0] +1
                    decision_dic[confidence_class_list[class_index]][1] = decision_dic[confidence_class_list[class_index]][1] + confidence_list[class_index]
                else:
                    decision_dic[confidence_class_list[class_index]] = [1,confidence_list[class_index]]
            vote_class = list(decision_dic.keys())  # vote()   # 找出投票最高的类
            vote_count = []
            for tmpclass in vote_class:
                vote_count.append(decision_dic[tmpclass][0])
            if max(vote_count) == min(vote_count):
                vote_score = []
                for tmpclass in vote_class:
                    vote_score.append(decision_dic[tmpclass][1])
                max_score_index = vote_score.index(max(vote_score))
                max_score_class = vote_class[max_score_index]
                return max_score_class
            else:
                max_vote_count_index = vote_count.index(max(vote_count))
                return vote_class[max_vote_count_index]
        elif len(decision_list)==2:
            if confidence_list[decision_list[0]]>confidence_list[decision_list[1]]:
                # print(confidence_class_list[decision_list[0]])
                return confidence_class_list[decision_list[0]]
            else:
                # print(confidence_class_list[decision_list[1]])
                return confidence_class_list[decision_list[1]]
        elif len(decision_list)==1:
            # print(confidence_class_list[decision_list[0]])
            return confidence_class_list[decision_list[0]]


        


svm_sort_score_list = [0.9995015156632645, 0.999470847953024, 0.999403692817545, 0.9993544039963917, 0.9992075132406556, 0.9991847949981186, 0.9991774861170515, 0.9991139736210661, 0.9990684943510324, 0.9990429667669, 0.9990418889594965, 0.998598687312435, 0.9985526728414517, 0.9984299358609494, 
    0.9983672743026104, 0.9982201559128615, 0.9981242835835067, 0.9981047987131626, 0.9980655605816862, 0.9980039917099397, 0.9979893745622787, 0.9975964511327292, 0.9975853830267194, 0.9973844502490953, 
    0.997343869405458, 0.9973355510283626, 0.9972550922019452, 0.9972169132950325, 0.9971837581359689, 0.9968657043252658, 0.9968459317688809, 0.9967729954366952, 0.9967144919229728, 0.9966585725892303, 0.9965325092518145, 0.9965091271005969, 0.9965081987963765, 0.9962349436877234, 0.9960313850078621, 0.9960267128609526, 0.9959309260248324, 0.9959151968417791, 0.9958738249210065, 
    0.9956773099011745, 0.9956761691607606, 0.9956749857598268, 0.995593589800974, 0.995569120192895, 0.995465783208341, 0.9953843559137382, 0.9952685148233353, 0.9952459201231795, 0.9949895587357324, 0.9949458221504637, 0.9948639525935649, 0.9948476315740532, 0.994707126076092, 0.9946067095623279, 0.9944646179284587, 0.9944180823091859, 0.9943635382089936, 0.9943105907418283, 0.9942902903752793, 
    0.9941318942525602, 0.994033023773543, 0.9940192761686889, 0.9939742127688733, 0.9939640690909065, 0.9939255555348071, 0.9938449468611937, 0.9937544962359135, 0.9937241796557285, 0.9936951136419385, 0.9932875387846998, 0.9931621421200345, 0.9930668772934017, 0.9930216924511558, 0.9929255606198346, 0.9928622425803402, 0.9928271075289383, 0.9928260787204193, 0.9926986169060463, 0.9926819388523468, 0.9926541222628048, 0.9926049253657105, 0.9925753992245843, 0.9925089102517208,
 0.9924704462866103, 0.9923980657295316, 0.9923343298819925, 0.9918607330319716, 0.9918397588639811, 0.9916423813204844, 0.9915517053886272, 0.9914260923136128, 0.991219170624239, 0.9911411117634666, 0.9910019726736314, 0.9909252578849237, 0.9908804538733345, 0.9907742893222727,
 0.9907349714454089, 0.9906373852177084, 0.9906062881950012, 0.9904795995437403, 0.9904230479156825, 0.9903748790626535, 0.9903150278795757, 0.9902016907038641, 0.990173993950412, 0.9901699703946115, 0.9900638072265908, 0.990034108942139, 0.9898392749383262, 0.9895621653486473, 0.9895371185064882, 0.9894861863443215, 0.9894804364564591, 0.9891965745024612, 0.9891637712296478, 0.9890496952127518, 0.9890494637248148, 0.989020557219106, 0.9888013517761964, 
 0.9887759599594819, 0.9886698565110777, 0.9883735591287691, 0.9883288931100134, 0.9882294445546038, 0.9880903497542477, 0.9880035180748491, 0.9878943911527032, 0.9878623293937704, 0.9878621537139578, 0.9878591294723299, 0.9878234705759475, 0.9878092189631719, 
    0.9875701625928677, 0.9875015483349622, 0.9875010083104525, 0.9874924355137856, 0.9874588951356417, 0.9873764860355075, 0.9873751713564307, 0.9873202269541883, 0.9870795986047589, 0.9870462456425853, 0.9870112970684837, 0.9869375379715305, 0.9869274109252779, 0.986864866664009, 0.9866982287232008, 0.9866374501580808, 0.9866227784977314, 0.9864998385877873, 0.986492698340869, 0.9863099150256759, 0.9863032758039239, 0.9861165413100387, 0.9860827910596556, 0.9858377588620145, 0.9857819956773266, 0.9856999240752752,
 0.9856418775676492, 0.9852418065652692, 0.9851114538904996, 0.9849754925321358, 0.9849645781500794, 0.9849593996933587, 0.9848415017039853, 0.9847736071953697, 0.9847250657558672, 0.9844913422426049, 
    0.984337594299404, 0.9842883593950245, 0.9840731370001604, 0.9839633715388331, 0.983931461167936, 0.9838579824827316, 0.9837902295002527, 0.9836510567977619, 0.9836358090980313, 0.98348312282068, 0.9834417090713553, 0.9833334494327497, 0.9832556900927877, 0.9832137806386533,
 0.9830936684757978, 0.9829604529836292, 0.9826680520924493, 0.9826218309601801, 0.9826013419922106, 0.9824356751512306, 0.9824095566427037, 0.9823947363796172, 0.9822668970598795, 0.9821962885842758, 0.982170561471038, 0.9821233136893518, 0.9814376405595303, 0.9814347448713399,
  0.9814014681150109, 0.981231856299658, 0.9811328801254081, 0.9807971247939337, 0.9806772171664597, 0.9803675689400118, 0.9803553357653155, 0.9802913151554624, 0.9801505884399595, 0.9800744147079485, 0.9798949859635059, 0.9798657474587064, 0.9797284962340315, 0.9795266813984969,
   0.979485025403893, 0.9794788808160568, 0.9793737114579494, 0.9792900871957296, 0.9789002064766674, 0.9785579421917129, 0.978515750434115, 0.9784532105019472, 0.9784005184117268, 0.9783331488247005, 0.9781691525375151, 0.9779572322135958, 0.9777839376053236, 0.977497336776419, 0.9773420071746115, 0.9773316994754783, 0.9772151898709633, 0.9771589480596314, 0.9771533612310916, 0.9771013678684743, 0.9769224270414507, 
    0.9768105728883775, 0.9767433614719978, 0.9765906231905701, 0.9759494608250571, 0.9759349453337203, 0.9759315023556969, 0.9758903986964182, 0.9758883099436657, 0.9758534295545204, 0.9757561564800558, 0.975745497553434, 0.9756496098708812, 0.9752995528194915, 0.9750540962901618, 0.9750458647447451, 0.9750323419174572, 0.9747448049257826, 0.9746268976405822, 0.9744601646076635,
 0.9742520085872713, 0.9741209709230895, 0.9738095183978805, 0.9737576574361002, 0.973633882144481, 0.9733644977129489, 0.9733191320220523, 0.9726682496446406, 0.9726557054056042, 0.9725044984642229, 0.972402186367968, 0.9721845459596175, 0.9713609050178404, 0.9712365653745211, 0.9711933840039264, 0.9711163788347179, 0.9710679272412598, 0.9705618715219527, 0.9704021163015171, 0.9702199026091934, 0.9700304361547454, 0.9696197645619998,
  0.9694293876207073, 0.969357618839458, 0.9693094411966526, 0.9691681941644348, 0.9690866137712079, 0.9690037129031754,
 0.9688606807409996, 0.9687863499354232, 0.9687397589557055, 0.9685351001381743,
 0.9684835551739108, 0.9684795662236838, 0.9682786743929811, 0.9681793112042689, 0.968066697509992, 0.9677182532299077, 0.9676091994343088, 0.9665504001886505, 0.9665324043254175, 0.9665008878048996, 0.9661383529580462, 0.9659721972129149, 0.9658831110712297, 0.9657458008779172, 0.9653319627708947, 0.9651332355993258, 0.9649652456003399, 0.9644249800839348, 0.9640592903660891, 0.9639972553352167, 0.963724146088377, 0.963571486607051, 0.9635445387363714, 0.9625661109114638, 0.9624577262978865, 0.9622580464743511, 
 0.9619393929530256, 0.961682072718677, 0.9616350288483008, 0.9615812600066519, 0.9613893816578823, 0.9610443227250297, 0.9609168168069987, 0.9604802634818924, 0.9604473587390412, 0.96017510462736, 0.9598253530900196, 0.9598008330945205, 0.959711174743752, 0.9596084086166521, 0.9594087837898282, 0.9593325772580868, 0.9592083541280692, 0.9591426545158542, 0.9583824288518543, 0.9583399616080936, 0.9582965513909723, 0.9582693125065417, 0.9580266541177448, 0.9577909491115484, 0.9577884137723865, 0.9576174550628908, 0.9573926377163343, 
 0.9573599101770357, 0.9570696604070197, 0.9566445523914048, 0.9566368492322712, 0.9560640199698262, 0.9558963269284366, 0.9555847602547077, 0.9552563037628499, 0.9551456680625214, 0.9550988396477242, 0.9545147021476532, 0.9544422506453771, 0.9537446511441856, 0.9536858927984772, 0.9534413057552568, 0.9532379612003639, 0.953011857430298, 0.9529706399091027, 0.9528482144998976, 0.9526836593559942, 0.9525809878526252, 0.9525332588766596, 0.9518357146261394, 0.9517348398278648, 0.9515793523273036, 0.9515608248283829,
  0.9515330489039747, 0.9514917417194662, 0.9514184700156867, 0.9513975534548292, 0.951389167330398, 0.9512931885566459, 0.9512323604118038, 0.9507399787338622, 0.9504439471867608, 0.9502365555026647, 0.9501642392267493, 0.9499691758756283, 0.9497097592517173, 0.9494040389779845, 0.9486086437539182, 0.9485264044282042, 0.9467737152276113, 0.9467619737657749, 0.9466378666797507, 0.9460334680487289, 0.9458484783972682, 0.9453661062875646, 
  0.9453277985068342, 0.9451979904051581, 0.945140031981138, 0.9450592585547276, 0.9448219733132484, 0.9427414724754137, 0.9427028631349456, 0.9422211121648841, 0.9415713677769495, 0.9412445805574037, 0.9410644384576993, 
    0.9406098790725392, 0.9400689279806285, 0.9398892992887222, 0.9394140595349103, 0.9391429418814841, 0.9379235005067078, 0.9377218114911592, 0.9371430020398503, 0.936126921095165, 0.9361042219388047, 0.9356359967642838, 0.9348274233716534, 0.9347428508181344, 0.9345314346025161, 0.9345126382048504, 0.934404358085506, 0.933968363036747, 0.9332002203575268, 0.9327175565199591, 0.9326444913252684, 0.9324492932046989, 0.9321269793000078, 0.9320130969158236, 0.9311217883177659, 0.929924094189474, 0.9288905270301444, 0.927911330495321, 0.9279074212037454, 0.92787100130667, 0.9266215577311999, 0.9264234512963782, 0.9263230828756474, 0.9257851362154507, 0.9257657168158181,
 0.925677595391685, 0.9251861280103044, 0.9246805019576052, 0.9244439480415619, 0.9238856819798048, 0.9233561532447857, 0.9229975016281587, 0.9229552011913562, 0.9226859636414814, 0.9222879416977773, 0.9216845090864859, 0.9214218014451365, 0.9211341447784901, 0.9208888522906832, 0.9204562993532163, 0.9195640299780277, 0.9194987524225053, 0.9191717379202312, 0.9189331254582186, 0.9183861270012073, 0.9182992234969028, 
    0.9171292437032511, 0.9167473760957611, 0.9150710053710112, 0.9141778805406788, 0.9136925895438927, 0.9134043286225805, 0.9133145515289877, 0.9129276071289409, 0.9127875712310155, 0.9123089362693729, 0.911904179984605, 0.9118006235496697, 0.9117295453047692, 0.9117033839821419, 0.9111763349376529, 0.9111403656120969, 0.910906520812708, 0.9106960849756438, 0.9104300605834801, 0.9098010822872544, 0.9088988108724851, 0.9087904639375111, 0.9085335729015305, 0.9081148714044774, 0.9074621831580881, 0.9068241605876516, 0.9060631151459094, 0.9059064631892543, 0.9046386770693077, 0.9037743411937245, 0.9025289671829958, 
    0.9016297910300557, 0.9011709485219067, 0.90093300812381, 0.899791065262649, 0.8988512542537026, 0.8987720628645388, 0.8981780423630802, 0.8977628510983462, 0.897382280823823, 0.896130085354402, 0.8941872211458625, 0.893363725794648, 0.8931947738072539, 0.8930513996973782, 0.8923914793373365, 
    0.8921585297727568, 0.8912517107058939, 0.8910725065848872, 0.8909262455696977, 0.8908266242147597, 0.8904929534441449, 0.8903445546032634, 0.8895706658336917, 0.889456482270897, 0.8883424036121284, 0.8877483958741617, 0.8877313149349652, 0.8875482692648272, 0.8872652859476237, 0.8872633185139225, 0.8869291231986489, 0.8860921565930897, 0.885773581793639, 0.8840735551785801, 0.8840529224529488, 0.8839072312029082, 0.8837184928196862, 0.8825802424658415, 0.8822225501973425, 0.8818804511130119, 0.8810536390037528, 0.880515689291194, 0.8796878041500868, 0.8789956148311662, 0.8783678382278347, 
    0.8776800000489463, 0.8770871437458024, 0.8756750424550369, 0.875239387482367, 0.8751149764060739, 0.8749966293432522, 0.8749095808256128,
 0.873927781763424, 0.8737605062434566, 0.8721775517247563, 0.8720089606767926, 0.871903181374324, 0.8718842446403646, 0.8708893704775719, 0.8706797139348798, 0.8706580770242355, 0.8706579904224974, 0.870151701324927, 0.8686118229639231, 0.8683631152488148, 0.86594856076297, 0.8654388537100748, 0.8649362020780827, 0.8641324361227631, 0.8634369668958383, 0.8629024187748714, 0.862774741695868, 0.8626638477732208, 0.8623653239180092, 0.8620220689528898, 0.8616084099137196,
  0.8609563264874371, 0.8589703380075865, 0.8588724547593085, 0.8582830044710696, 0.8576624090977095, 0.8574175889924348, 0.8533866554173851, 0.8530526043131337, 0.8530286629437125, 0.852772866260447, 0.8517820736750591, 0.8509184058819682, 0.8502794066346578, 0.8500191672992483, 0.8482031712476672, 0.8479486092213321, 0.8471936764140883, 0.8458737219262531, 0.8457140223333817, 0.8452507225833666, 0.8452210449347062, 0.8447867218951916, 0.8440222420845594, 0.8431050491066829, 0.8427369122140353, 0.8425124557255028, 0.841833788685224, 0.8415812274085173, 0.8397735108807922, 0.8380810898950399, 0.8379704474586833, 0.8378113234164701, 
  0.8364071754180862, 0.8363730376919233, 0.834880416889903, 0.8341831290896342, 0.8322695355945531, 0.8296889907836121, 0.828651500666776, 0.8272830534498006, 0.8265426507647876, 0.8264442124699601, 0.8248220739250006, 0.8246628820232115, 0.8246580540639914, 0.8245052009270522, 0.8238922672922075, 0.8232503525936377, 0.8217191401614873, 0.8216289345040978, 0.8196932387226593, 0.8186687833007391, 0.817923169052015, 0.8168248958643088, 0.8163523413644163, 0.8155526993750003, 
    0.8154497720455929, 0.815107269694899, 0.815075412760806, 0.8142491264114858, 0.814179886074365, 0.8137955675492371, 0.8121054177397034, 0.8093276484052765, 0.8092061590274462, 0.8081365249969993, 0.8077931214110448, 0.8062176340653622, 0.8042272802479457, 0.8032855324352893, 0.8029382134513486, 0.8026353432484822, 0.8002920238003447, 0.799883992082669, 0.7988427049764032, 0.7979028138801706, 0.7976416731107031, 0.7966745493261137, 0.7963582164965964, 0.7953872836944121, 0.7924305595952151, 0.7917752104051122, 0.7914090212458303, 0.7904639361517037, 0.789270408798649, 0.7887920603615949, 0.7878371781579894, 0.7864582238570081, 0.7787781409536931, 0.7787059888159146, 0.7761742666570302, 0.7760181409925259, 0.7758347177420788, 0.7743445766314725, 0.7732195508153097, 0.7719530900320447, 0.7707313045260146, 0.7702656015495278, 0.7700686241022563, 0.7683409456883762,
 0.7666838737065714, 0.7663254287943349, 0.7657441811410264, 0.7656435848377635, 0.765629143134506, 0.7645597519330929, 0.763785584496219, 0.762220248175084, 0.7613708637512335, 0.7561579816586964, 0.7552128381002766, 0.7541592296330404, 0.7531833929095427, 0.7520513695254155, 0.7513738976204956, 0.7475273343197467, 0.7454837388220573, 0.7453089011078833, 0.7423047371994135, 0.7416955660302464, 0.7398604533433135, 0.7361685472138262, 0.7317428833250852, 0.7308542193867927, 0.7274329296135968, 0.726887663284634,
 0.7250350667818067, 0.7246435228487902, 0.7241600507396342, 0.7189224873523358, 0.7185466621369718, 0.7183596456314022, 0.7161081361325172, 0.7125969116438329, 0.7125576775383814, 0.7105903294276128, 0.7103499188718873, 0.7097176056629869, 0.7074989193151779, 0.7068617242480415, 0.7026956124040119, 0.7025766555924062, 0.702511287793785, 0.702119522745476, 0.7002545809979623, 0.6975246639159888, 0.6972096571016588, 0.6966978014851937, 0.6958469321156432, 0.6956500890155051, 0.6955193364484561, 0.6946008533986067, 0.6907005061677378, 0.6883957132921514, 0.687720028622423, 0.6876930410602747, 
 0.686628097999218, 0.6857078529155979, 0.6851994233030975, 0.6841896296264753, 0.6823412516982507, 0.6820857907517276, 0.678968751028941, 0.678968751028941, 0.6784030448121862, 0.6781152678858497, 0.6745103469870573, 0.672076411494493, 0.6715329610892945, 0.6714923491684708, 0.6701979211884159, 0.6685239658107334, 0.6682544615265987, 0.6680734035939363, 0.6672242100581202, 0.6642784424247807, 0.6633152758385055, 0.6619077740347288, 0.6617810858323486, 0.6609959025136186, 0.6594829468473871, 0.6593704648831612, 0.657437139104077, 0.6561727813018946, 0.6508285830020263, 0.6498225015875464, 0.6483733197662513, 0.6446408716790614, 0.644005990339252, 0.6432298090029929, 0.6416944819467845, 0.6413171778528178, 0.6404202579882203, 
 0.6401389831044927, 0.6378110371034795, 0.6331710988231826, 0.6323723050982356, 0.6318872429901027, 0.6298715967254338, 0.6293546871341253, 0.628601409847375, 0.6278084208641852, 0.6274277583041985, 0.6244518638114965, 0.6232229104102225, 0.6221676998100728, 0.6218184899438228, 0.619808412066279, 0.6190177673421456, 0.6190073993602228, 0.6187690964526323, 0.6185158824439088, 0.6170051581978515, 0.6164806785014804, 0.6163483199281728, 0.6144363146803047, 0.6086799239100502, 0.6074297395674235, 0.6069954255303012, 0.6068539677182286, 0.6041486037101599, 0.6012392149128716, 0.6006804766109832, 0.5978241356670954, 0.5977020703735417, 0.5966052421479847, 0.5918848632150846, 0.5914485897787027, 0.5834924351361588, 0.5825453737520916, 0.5763079029805112,
  0.5757006993027136, 0.5749269145614156, 0.5722905476413439, 0.569010024586094, 0.5673496295779124, 0.5615636160821572, 0.5606477991937965, 0.5596834676489532, 0.5582144340983916, 0.5536618370546279, 0.5528636523464528, 0.5509564040540673, 0.5474775702130464, 0.5461528756768237, 0.5417474017053541, 0.5385286438513311, 0.5348337268029079, 0.5331023100299427, 0.5291731629172669, 0.5269664632494693, 0.5227626566262279, 0.5218253829128207, 0.5190104035708765, 0.5143288150564944, 0.5134712318126958, 0.5114297389027828, 0.5080896240851716, 0.5029125980564524, 0.49889951426430723, 0.4949100639010387, 0.4939058178700357, 0.4932527587693938, 0.4913415371734682, 0.4873881110907765, 0.4851491962163822, 0.48492143398820386, 0.48309427733610705, 0.4804706156110708, 0.4772714196756036, 0.4756533012123156, 0.47517238237942516, 0.4745905030974321, 0.4681635500755355, 0.4671077251282272,
   0.46563621478197836, 0.4655562528734392, 0.4651683700388872, 0.46452041203834304, 0.45713031533305615, 0.45590184474068585, 0.4555237547100175, 0.4550092388077338, 0.44837430790768357, 0.44834496154287135, 0.4474660112716207, 0.4453891786812168, 0.44076865438648416, 0.4405471352108393, 0.4396127424534833, 0.4393161243689533, 0.4371598768258826, 0.430677319616193, 0.42769586152825795, 0.4265293476173468, 0.42352384609226634, 0.4222039650647897, 0.418071119698517, 0.41356969908675784, 0.41256264990041747, 0.4104924873816682, 0.41047764240297097, 0.4051677878753825, 0.40049837522435194, 0.39682758522771605, 0.39511232094791193, 0.39360881094538924, 0.38618899986440863, 0.38437917003123656, 0.38374296493958787, 0.3822714213472906, 0.373062524276334, 0.3689377640665318, 0.3611374188566826, 0.35762678747980237, 0.3510087224189998, 0.3499718419581102, 0.34608688118691805, 0.3441664846099609, 0.3388122208758266, 0.33726323903181715, 0.33592989058322636, 
   0.33184675860203644, 0.33156742508030695, 0.3309535550021768, 0.32751448740798356, 0.3251945191764642, 0.3235328950606971, 0.32228133474555554, 0.3189977657573469, 0.3178627270058701, 0.31171702218744257, 0.31154923149078684, 0.30603051236479467, 0.30516705609855616, 0.30450606783076184, 0.30402115034626287, 0.30250201362360346, 0.3015684138135382, 0.2988332344720104, 0.2985441243316119, 0.2950713513380151, 0.29314594225536394, 0.28648198545894377, 0.2848167152217239, 0.28444957669965737, 0.2842901831613947, 0.28394707424417187, 0.2833789904094455, 0.2816696241843342, 0.27705821386329144, 0.2678574978730334, 0.2564776432587542, 0.2524962716801391, 0.2514182263824396, 0.24434620833311438, 0.24420193872058957, 0.24311648149087614, 0.24268991660031167, 0.24253286449026198, 0.23694804290568114, 0.23528930972926757, 0.2261120204996855, 0.21949845051807848, 0.21919389388336347, 0.21440185802721906, 0.20941204430383697, 0.2046517029200686,
 0.20288253901358277, 0.20104443476998496, 0.19883960033009906, 0.1925852674959834, 0.1917465084719264, 0.1889180871053667, 0.18646402425003517, 0.18393774137502697, 0.18287356613637987, 0.18204364689653393, 0.18116915431337627, 0.17425388838912437, 0.1712274694199733, 0.16736350648654827, 0.16730591276811527, 0.1599561684482882, 0.15355483382782556, 0.15124753542400693, 0.1508562976962287, 0.14987027663582256, 0.1375987569808999, 0.13507002857048053, 0.13100825234229208, 0.13028078522226585, 0.12873885219444664, 0.12863248013038495, 0.12624155075976595, 0.12228595906017597, 0.1216426645316963, 0.1187628105767869, 0.11794238949036759, 0.11574392324315798, 0.1117045187449998, 0.10680813163744736, 0.10583455172868629, 0.1044332210411737, 0.10113672116895724, 0.09894193846502387, 0.09751662216768052, 0.09722268952441177, 0.09700328932068152, 0.09671448368814856, 0.09547829149628684, 0.09388263907206591, 0.09148192609160014, 0.08454731897197366, 0.08178010488474592, 
 0.07997319549202582, 0.07679051409898253, 0.07669077076574554, 0.07632877018632143, 0.07113742116719853, 0.07112355960122803, 0.06505119401138092, 0.06422328871459856, 0.06397108911052715, 0.06366138078034245, 0.06294221632356849, 0.06263497117952821, 0.06230579513741526, 0.061633092255172814, 0.06133159031407897, 0.061193429961107326, 0.060887822327552604, 0.06078116666939185, 0.060486496161569175, 
    0.05961655201326374, 0.05955323583914245, 0.0590267423289618, 0.05782009070234854, 0.05759498187170188, 0.057108487315125696, 0.0567795731308989, 0.05554516286373332, 0.050888123356894084, 0.04887507662413659, 0.04874992299826147, 0.04853428353595486, 0.04672756094270374, 0.04655760468805471, 0.045852600063755626, 0.04573760615010333, 0.04353410190238976, 0.04317570300979189, 0.04309503867600938, 0.04256100266912241, 0.03835226761193631, 0.03620447306306202, 0.03616378475037736, 0.035719223158891825, 0.035246239108545206, 0.03462519795165651, 0.03440857715692818, 0.03424849048095721, 0.033954286681467376, 0.033284605321184976, 0.033257685687790486, 0.03207110557430887, 0.031878067304931826, 0.030339224735927515, 0.029569775015074955, 0.02787061184086865, 0.02683226243647538, 0.026739451123407303,
 0.02604570341784441, 0.02265477338292765, 0.021214928699306777, 0.02051585813310075, 0.019032466522990975, 0.01784373301876357, 0.0122977077458478, 0.01164840116479124, 0.01138762405467908, 0.00913734045892297]



svm_confidence_list= [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419,
  0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419,
  0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
   1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 
  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 
  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 
  0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 1.0, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871,
   0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 
  0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 
  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871,
   0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871,
    0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 
    1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 
    0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419,
     0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 
     1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
     1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.8709677419354839, 
     0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8387096774193549, 0.8387096774193549, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419,
      0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.9354838709677419, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 1.0, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.9354838709677419, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 
      0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.7741935483870968, 0.7741935483870968, 0.8064516129032258, 
      0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.8709677419354839, 0.9032258064516129, 0.8709677419354839, 0.8387096774193549, 0.8387096774193549, 
      0.8387096774193549, 0.8387096774193549, 0.8709677419354839, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129,
       0.9032258064516129, 0.9354838709677419, 0.9354838709677419, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.967741935483871, 0.9354838709677419, 
       0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.8709677419354839, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8387096774193549, 0.8387096774193549, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.7741935483870968,
        0.7741935483870968, 0.7419354838709677, 0.7096774193548387, 0.7096774193548387, 0.7096774193548387, 0.7419354838709677, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7419354838709677, 0.7419354838709677, 0.7096774193548387, 0.7096774193548387, 0.7096774193548387, 0.7096774193548387, 0.7096774193548387, 0.7419354838709677, 0.7419354838709677,
         0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.8064516129032258, 0.8064516129032258, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8064516129032258, 0.8387096774193549, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8387096774193549, 0.8387096774193549, 0.8709677419354839, 
    0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8064516129032258, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.8064516129032258, 0.8064516129032258, 0.8387096774193549, 0.8709677419354839, 
    0.8709677419354839, 0.8709677419354839, 0.8387096774193549, 0.8387096774193549, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8387096774193549, 0.8709677419354839, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.9032258064516129, 0.8709677419354839, 0.9032258064516129, 0.9032258064516129,
    0.9032258064516129, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8387096774193549, 0.8709677419354839, 0.8709677419354839, 0.9032258064516129, 0.9032258064516129, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8709677419354839, 0.8387096774193549, 0.8387096774193549, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.8064516129032258, 0.7741935483870968, 0.7741935483870968, 0.7419354838709677, 
    0.7419354838709677, 0.7419354838709677, 0.7419354838709677, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.8064516129032258, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7419354838709677, 0.7419354838709677, 0.7741935483870968, 0.7741935483870968, 0.7419354838709677, 0.7419354838709677, 0.7419354838709677, 0.7419354838709677, 0.7096774193548387, 0.7096774193548387, 0.7096774193548387, 0.7419354838709677, 0.7419354838709677, 0.7419354838709677,
    0.7419354838709677, 0.7096774193548387, 0.7096774193548387, 0.7419354838709677, 0.7419354838709677, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7741935483870968, 0.7419354838709677, 0.7096774193548387, 0.7096774193548387, 0.7096774193548387, 0.7419354838709677, 0.7096774193548387, 0.6774193548387096, 0.7096774193548387, 0.7096774193548387, 0.7096774193548387, 0.7096774193548387, 0.7419354838709677, 0.7419354838709677, 0.7419354838709677, 0.7741935483870968,
    0.8064516129032258, 0.8064516129032258, 0.7741935483870968, 0.7419354838709677, 0.7096774193548387, 0.7419354838709677, 0.7419354838709677, 0.7741935483870968, 0.7741935483870968, 0.7419354838709677, 0.7096774193548387, 0.6774193548387096, 0.6774193548387096, 0.6451612903225806, 0.6451612903225806, 0.6451612903225806, 0.6774193548387096, 0.7096774193548387, 0.7096774193548387, 0.6774193548387096, 0.6451612903225806, 0.6451612903225806, 0.6774193548387096, 0.6774193548387096, 0.6774193548387096, 
    0.6774193548387096, 0.6451612903225806, 0.6129032258064516, 0.6129032258064516, 0.6129032258064516, 0.6129032258064516, 0.5806451612903226, 0.5483870967741935, 0.5483870967741935, 0.5483870967741935, 0.5483870967741935, 0.5161290322580645, 0.4838709677419355, 0.45161290322580644, 0.41935483870967744, 0.45161290322580644, 0.4838709677419355, 0.4838709677419355, 0.4838709677419355, 0.4838709677419355, 0.4838709677419355, 0.45161290322580644, 0.41935483870967744, 0.41935483870967744, 0.3870967741935484,
    0.3870967741935484, 0.3870967741935484, 0.3870967741935484, 0.3548387096774194, 0.3225806451612903, 0.3225806451612903, 0.2903225806451613, 0.2903225806451613, 0.3225806451612903, 0.2903225806451613, 0.25806451612903225, 0.22580645161290322, 0.22580645161290322, 0.22580645161290322, 0.25806451612903225, 0.25806451612903225, 0.25806451612903225, 0.25806451612903225, 0.25806451612903225, 0.25806451612903225, 0.2903225806451613, 0.25806451612903225, 0.25806451612903225, 0.25806451612903225, 0.22580645161290322, 
    0.22580645161290322, 0.1935483870967742, 0.1935483870967742, 0.22580645161290322, 0.1935483870967742, 0.1935483870967742, 0.1935483870967742, 0.1935483870967742, 0.1935483870967742, 0.1935483870967742, 0.1935483870967742, 0.16129032258064516, 0.16129032258064516, 0.16129032258064516, 0.12903225806451613, 0.12903225806451613, 0.12903225806451613, 0.12903225806451613, 0.12903225806451613, 0.12903225806451613, 0.0967741935483871, 0.12903225806451613, 0.12903225806451613, 0.12903225806451613, 0.12903225806451613, 
    0.12903225806451613, 0.0967741935483871, 0.0967741935483871, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871,
    0.12903225806451613, 0.12903225806451613, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.0967741935483871, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 
    0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.06451612903225806, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 
    0.03225806451612903, 0.03225806451612903, 0.03225806451612903, 0.03225806451612903]








# svm_struct====================================
svm_score_list = []
svm_predict_list = []
# all_class_list = []
# svm_y_test


for i in range(len(pred_y_struct)):
    indextmp = pred_y_struct[i]
    svm_score_list.append(pred_y_proba_struct[i][indextmp])
    svm_predict_list.append(indextmp)


sort_list = []
for i in range(len(svm_score_list)):
    tmplist = []
    tmplist.append(svm_score_list[i])    # 预测得分
    tmplist.append(svm_predict_list[i]) # 预测类别
    tmplist.append(svm_y_test[i])        #实际类别
    sort_list.append(tmplist)


# 按预测得分大小排序
def takeFirst(elem):
    return elem[0]

sort_list.sort(key=takeFirst,reverse = True)
print("sort_list")



#   2 排序完计算置信度
svm_sort_score_list = []  # 横坐标
for tmp_list in sort_list:
    svm_sort_score_list.append(tmp_list[0])

svm_sort_predict_list = []
for tmp_list in sort_list:
    svm_sort_predict_list.append(tmp_list[1])

svm_sort_class_list = []
for tmp_list in sort_list:
    svm_sort_class_list.append(tmp_list[2]) 


svm_count_true_list = []    # 按顺序统计正确预测的数量
count_number = 0
for i in range(len(svm_sort_score_list)):
    if svm_sort_predict_list[i] == svm_sort_class_list[i]:
        count_number = count_number + 1
    svm_count_true_list.append(count_number)


print("svm_count_true_list")
print(svm_count_true_list)  #   排序完的数组


confidence_n = 100  #


svm_confidence_list = []
# 获得置信度
for i in range(len(svm_sort_score_list)):
    tmpcount = 0
    firstpart = 0
    lastpart = 0
    if i<confidence_n + 1:
        firstpart =  confidence_n - i + svm_count_true_list[i]
    else:
        firstpart = svm_count_true_list[i] - svm_count_true_list[i - confidence_n -1]
    if i + confidence_n >= len(svm_sort_score_list):
        lastpart = svm_count_true_list[len(svm_sort_score_list)-1] - svm_count_true_list[i]
    else:
        lastpart = svm_count_true_list[i + confidence_n] - svm_count_true_list[i]
    tmpconfidence = (firstpart + lastpart)/(2*confidence_n + 1)
    svm_confidence_list.append(tmpconfidence)

print("svm_confidence_list")
print(svm_confidence_list)

